# ğŸ“¦ INSTALLATION.md

## ğŸš€ Distributed K-Means with Apache Spark

This guide walks you through the full setup and execution of a distributed K-Means clustering pipeline using:

- Apache Spark (via Bitnami Docker images)
- Docker Compose (multi-container setup)
- The SUSY dataset (2.4 GB from UCI ML repository)

## ğŸ› ï¸ Requirements

Before starting, ensure the following are installed:

| Tool           | Version        |
|----------------|----------------|
| Docker         | â‰¥ 20.10        |
| Docker Compose | â‰¥ v2.0         |
| Python         | â‰¥ 3.8 (for local runs) |
| wget / curl    | optional (for dataset download) |

---

## ğŸ“ Project Structure

```

.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kmeans\_job.py         # Main KMeans runner
â”‚   â””â”€â”€ utils.py              # Dataset stat & logging tools
â”œâ”€â”€ data/
â”‚   â””â”€â”€ SUSY.csv              # Dataset file (\~2.4 GB)
â”œâ”€â”€ results/                  # Output logs + metrics
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run\_once.sh           # Runs one cluster config
â”‚   â”œâ”€â”€ run\_cluster.sh        # Runs all worker configs
â”‚   â”œâ”€â”€ run\_local.sh          # Run KMeans locally (no Spark)
â”‚   â””â”€â”€ cleanup.sh            # Teardown & cleanup

````



## ğŸ§¾ Step-by-Step Installation

### 1. Clone or Prepare the Project

```bash
git clone <your-repo-url>
cd <project-dir>
````

---

### 2. Download the SUSY Dataset

You can either download manually or let the script fetch it:

```bash
wget https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz -P data/
gunzip data/SUSY.csv.gz
```

Ensure the resulting path is: `data/SUSY.csv`.

---

### 3. Build Docker Images

```bash
docker compose build
```

This compiles the Spark master and worker containers based on your Dockerfile.

---

### 4. Start Cluster (e.g., 2 Workers)

```bash
WORKERS=2 docker compose up -d --scale spark-worker=2
```

This starts the Spark master and 2 worker nodes in detached mode.

---

### 5. Run Distributed KMeans Job

```bash
docker compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  /workspace/src/kmeans_job.py \
  --k 8 --maxIter 20 --parts 16 --fraction 1.0 --tag run_2workers
```

This uses the full dataset and saves metrics to `results/`.

---

## ğŸ§ª Benchmark: Run at 1/2/5 Workers

To benchmark all configs with a single script:

```bash
./scripts/run_cluster.sh
```

To run a single config:

```bash
./scripts/run_once.sh 5 --k 8 --maxIter 20 --parts 16 --fraction 0.5
```

---

## ğŸ§¼ Cleanup

```bash
./scripts/cleanup.sh
```

This stops and deletes all containers and volumes used in the test.

---

## ğŸ“Š Output

All job metrics are saved in structured JSON format in the `results/` directory:

```
results/
â”œâ”€â”€ run_1w_20250621_2015.json
â”œâ”€â”€ run_2w_20250621_2041.json
â””â”€â”€ ...
```

Each file contains:

* Row count
* Feature count
* Elapsed time
* Clustering cost (WSSSE)
* Cluster size distribution

---

## ğŸ–¥ï¸ Spark UI

Access the live Spark job dashboard via:

```
http://localhost:18081
```

Useful for debugging, executor tracking, and job visualization.


âœ… Success Criteria

* Can run on 8 GB RAM machine (using `--fraction 0.5`)
* Full data = 5M rows, \~2.39 GB
* Each run logs performance + cluster metrics

